{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practical 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Using PySaprk  and RDD in Google Collab Jupiter Notebook"
      ],
      "metadata": {
        "id": "5vNMXdfDwoqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "D3Z2Wnmpixas"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" \n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "L6gSro404fUE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oz292WE5hxj",
        "outputId": "36e88dcc-0d8c-495d-e9db-f24b65d2fc03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "# If we are using sparksession 1st time we have to use sparksession.builder\n",
        "#sparksession allow all API ,spark context, sql context, hive context, spark stremming\n",
        "#spark directly allow to use sql . for that create sql"
      ],
      "metadata": {
        "id": "4A5cVsZC46_z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#you can create multiple spark session with buildet method . but context only create once for application not allow you to create for next time for same application.\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "id": "ItoqjaXs5Vjq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "297bf4ea-e9df-437e-c70d-3911376bc399"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f45d13e00d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://662288de10c3:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "DsGTNhTm5umj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c9819f-a53c-48d7-c773-bd5731ba7405"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "Pb3sOmGOmz30"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating spark context-Its like connecting to spark cluster\n",
        "from pyspark import SparkConf \n",
        "from pyspark.context import SparkContext"
      ],
      "metadata": {
        "id": "YZ-jXFti6Qzj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
      ],
      "metadata": {
        "id": "M5hHmrV36uXC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display details of sc\n",
        "sc"
      ],
      "metadata": {
        "id": "e7xw6Ik97ENS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "9f53eaed-567f-4da0-cbff-a8a4f0b51793"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://662288de10c3:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for prime numbers\n",
        "\n",
        "def isprime(n):\n",
        "    \"\"\"\n",
        "    check if integer n is a prime\n",
        "    \"\"\"\n",
        "    # make sure n is a positive integer\n",
        "    n = abs(int(n))\n",
        "    # 0 and 1 are not primes\n",
        "    if n < 2:\n",
        "        return False\n",
        "    # 2 is the only even prime number\n",
        "    if n == 2:\n",
        "        return True\n",
        "    # all other even numbers are not primes\n",
        "    if not n & 1:\n",
        "        return False\n",
        "    # range starts with 3 and only needs to go up the square root of n\n",
        "    # for all odd numbers\n",
        "    for x in range(3, int(n**0.5)+1, 2):\n",
        "        if n % x == 0:\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "fAoerxYZ7mBj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD of numbers from 0 to 1,000,000\n",
        "nums = sc.parallelize(range(1000000))\n",
        " \n",
        "# Compute the number of primes in the RDD\n",
        "print(nums.filter(isprime).count())"
      ],
      "metadata": {
        "id": "ejwkBj5l9GWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7812637-f557-49b8-c189-e2560de7111d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create Sparksession\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark1 = SparkSession.builder.master(\"local[1]\").appName(\"sparksession1\").getOrCreate()\n",
        "#getorcreate() object create sparksession object\n",
        "#local[1]-- to run satndalone can't be 0 -- the integer is always greater than 1"
      ],
      "metadata": {
        "id": "rjOuVhtI9SLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark1"
      ],
      "metadata": {
        "id": "MVtfc083CYoq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "c7080fff-29a8-4893-b85c-6a4113d69872"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f45d14019d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://662288de10c3:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new session\n",
        "Spark2=SparkSession.newSession\n",
        "Spark2"
      ],
      "metadata": {
        "id": "ZdM6Kgy-DJ96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62d1b252-8f93-4edc-889f-4ceaed466a0c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function pyspark.sql.session.SparkSession.newSession>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get existing session\n",
        "Spark3 =SparkSession.builder.getOrCreate\n",
        "Spark3"
      ],
      "metadata": {
        "id": "4Jim3tpGGkh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e26a294-28c7-4243-888f-3038e5cb4ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method SparkSession.Builder.getOrCreate of <pyspark.sql.session.SparkSession.Builder object at 0x7fc4eac08910>>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example: Create RDD to load the contents of text file and do the text analysis\n",
        "\n",
        "text = sc.textFile(\"/content/ex.txt\")\n",
        "text"
      ],
      "metadata": {
        "id": "vzM4_vHnGlyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6e8de8-e182-48e1-b32d-aaf3eceaf337"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/ex.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "def tokenize(text):\n",
        "     return text.split()"
      ],
      "metadata": {
        "id": "PlFhlMVrHJ6k"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.flatMap(tokenize)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "Hc3FzTGFIUf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3de0810-9365-43c4-f397-114aa198b07b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[4] at RDD at PythonRDD.scala:53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wc = words.map(lambda x: (x,1))\n",
        "wc"
      ],
      "metadata": {
        "id": "EuY1NcjuIXkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c1ddf31-e268-4c17-acbe-c3c0ada68189"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[5] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wc.toDebugString())"
      ],
      "metadata": {
        "id": "rc_GQpqVI5K0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "outputId": "736dcb23-ba46-4ce8-8652-83a263f07bdb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-9e18551da084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDebugString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtoDebugString\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2542\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mits\u001b[0m \u001b[0mrecursive\u001b[0m \u001b[0mdependencies\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2543\u001b[0m         \"\"\"\n\u001b[0;32m-> 2544\u001b[0;31m         \u001b[0mdebug_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDebugString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug_string\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2546\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdebug_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o93.toDebugString.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/ex.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.RDD.firstDebugString$1(RDD.scala:1950)\n\tat org.apache.spark.rdd.RDD.toDebugString(RDD.scala:1984)\n\tat org.apache.spark.api.java.JavaRDDLike.toDebugString(JavaRDDLike.scala:603)\n\tat org.apache.spark.api.java.JavaRDDLike.toDebugString$(JavaRDDLike.scala:602)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.toDebugString(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "counts = wc.reduceByKey(add)\n",
        "counts \n",
        "counts.saveAsTextFile(\"wcc\")"
      ],
      "metadata": {
        "id": "EkFNHjXNIiG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls wcc/"
      ],
      "metadata": {
        "id": "HNPwCMgqIlmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head wcc/part-00000"
      ],
      "metadata": {
        "id": "7HmBuXkqIr0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Date=12-07-22\n",
        "from pyspark.sql import SparkSession, SQLContext\n"
      ],
      "metadata": {
        "id": "BKfv6ePAkFFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SparkSess = SparkSession.builder.master(\"local[1]\").appName(\"mtApp\").getOrCreate()"
      ],
      "metadata": {
        "id": "U15O2H1GoTQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(SparkSess.sparkContext)\n",
        "print(SparkSess.sparkContext.appName)"
      ],
      "metadata": {
        "id": "pS1y6baTork_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SparkSess.sparkContext.stop()"
      ],
      "metadata": {
        "id": "ozJAlKylo6jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a list of roll number and name\n",
        "st =[(25,'Sarita'),(10,'Akshata'),(20,'Kajal'),(30,'Priti')]"
      ],
      "metadata": {
        "id": "6lZ0vu_Yo_x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "sts = sc.parallelize(st)"
      ],
      "metadata": {
        "id": "ldXSYuxBpZBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sts)"
      ],
      "metadata": {
        "id": "hbLDhapXp23l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns =['Roll_No','Name']"
      ],
      "metadata": {
        "id": "0blIEYFFqTDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = SparkSess.createDataFrame(data=st,schema=columns)"
      ],
      "metadata": {
        "id": "VpxTgb6lqb1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "vMGhhDl6qotd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.collect()"
      ],
      "metadata": {
        "id": "oGhhsNNwr0Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "ucK7iChJr-we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.first()"
      ],
      "metadata": {
        "id": "Qoa_uIAqtzw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st1 =[{'Name':'ABC','Roll_no':25},{'Name':'XYZ','Roll_no':10},{'Name':'PRQ','Roll_no':20},{'Name':'RTY','Roll_no':22}]"
      ],
      "metadata": {
        "id": "z_fjUp2Ut2T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st1"
      ],
      "metadata": {
        "id": "3WA9wON5xnrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = SparkSess.createDataFrame(st1)\n"
      ],
      "metadata": {
        "id": "oS-wv9G5t5e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "a4_NXEmTubQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfcsv = SparkSess.read.csv(\"/content/st1.csv\")"
      ],
      "metadata": {
        "id": "ibQFAzwEyzEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfcsv"
      ],
      "metadata": {
        "id": "Lq83LJsq0JD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfcsv.show()"
      ],
      "metadata": {
        "id": "tgQPkp4Y0MJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can try this format-csv,avro,parquet,tsv,xml,text,json"
      ],
      "metadata": {
        "id": "TNczDmYg0YpF"
      }
    }
  ]
}